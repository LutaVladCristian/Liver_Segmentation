{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LutaVladCristian/Liver_Segmentation/blob/master/colab_notebooks/train_nets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "90627602666a6128"
      },
      "cell_type": "markdown",
      "source": [
        "##### Run cell only in GoogleColab"
      ],
      "id": "90627602666a6128"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Mrd0TKmS3CQd",
        "outputId": "adf58f0b-249b-4491-8759-2601cbc53fbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Mrd0TKmS3CQd",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Installing dependencies (run cell only in GoogleColab)"
      ],
      "metadata": {
        "id": "6RomPqxm_6mC"
      },
      "id": "6RomPqxm_6mC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install monai and torch\n",
        "!pip install monai\n",
        "!pip install torch"
      ],
      "metadata": {
        "id": "WnzGkvbXULAj",
        "outputId": "361cccb4-0aeb-4434-c900-342f5a0d4fb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "WnzGkvbXULAj",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting monai\n",
            "  Downloading monai-1.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.11/dist-packages (from monai) (2.0.2)\n",
            "Requirement already satisfied: torch<2.7.0,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from monai) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7.0,>=2.4.1->monai)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7.0,>=2.4.1->monai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7.0,>=2.4.1->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<2.7.0,>=2.4.1->monai) (3.0.2)\n",
            "Downloading monai-1.5.0-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, monai\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed monai-1.5.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb3a5df9",
      "metadata": {
        "id": "cb3a5df9"
      },
      "source": [
        "#### In this Jupyter Notebook we will display the results after training of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96000eda",
      "metadata": {
        "id": "96000eda"
      },
      "source": [
        "##### Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "id": "ea815e7f",
      "metadata": {
        "id": "ea815e7f"
      },
      "source": [
        "import os\n",
        "from os.path import exists\n",
        "from glob import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from monai.networks.nets import UNet\n",
        "from monai.networks.layers import Norm\n",
        "from monai.losses import DiceLoss, TverskyLoss, DiceFocalLoss\n",
        "from monai.data import Dataset, CacheDataset, DataLoader\n",
        "from monai.utils import set_determinism\n",
        "\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        "    LoadImaged,\n",
        "    EnsureChannelFirstd,\n",
        "    ScaleIntensityRanged,\n",
        "    RandAffined,\n",
        "    RandFlipd,\n",
        "    RandGaussianNoised,\n",
        "    CropForegroundd,\n",
        "    Orientationd,\n",
        "    Resized,\n",
        "    ToTensord,\n",
        "    Spacingd,\n",
        "    EnsureTyped,\n",
        ")\n",
        "\n",
        "from monai.data.image_reader import NibabelReader"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "id": "6cb9bdee",
      "metadata": {
        "id": "6cb9bdee"
      },
      "source": [
        "##### Setting the path to the working directories"
      ]
    },
    {
      "cell_type": "code",
      "id": "790dbd62",
      "metadata": {
        "id": "790dbd62",
        "outputId": "8b0271b3-89cb-44fc-dd45-be24394185af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The input paths for the prepared nifti files\n",
        "nif_path = ['drive/MyDrive/data_set_group_nif/nif_files_testing/images',\n",
        "            'drive/MyDrive/data_set_group_nif/nif_files_testing/labels',\n",
        "            'drive/MyDrive/data_set_group_nif/nif_files_training/images',\n",
        "            'drive/MyDrive/data_set_group_nif/nif_files_training/labels',]\n",
        "\n",
        "print(nif_path[0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/MyDrive/data_set_group_nif/nif_files_testing/images\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "7f290f77ce837ce1"
      },
      "cell_type": "markdown",
      "source": [
        "##### Define the function for data preprocessing"
      ],
      "id": "7f290f77ce837ce1"
    },
    {
      "metadata": {
        "id": "8d8f5fc427d9fc7e"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 5,
      "source": [
        "def preprocess_data(data_path, batch_size=8, spatial_size=(256, 256, 16), pixdim=(1.5, 1.5, 2.0)):\n",
        "\n",
        "    set_determinism(seed=0)\n",
        "\n",
        "    # Create the dataset\n",
        "    test_data = sorted(glob(data_path[0] + f'/*'))\n",
        "    test_labels = sorted(glob(data_path[1] + f'/*'))\n",
        "\n",
        "    train_data = sorted(glob(data_path[2] + f'/*'))\n",
        "    train_labels = sorted(glob(data_path[3] + f'/*'))\n",
        "\n",
        "    train_files = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_data, train_labels)]\n",
        "    test_files = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(test_data, test_labels)]\n",
        "\n",
        "    # Transforms for the training with data augmentation\n",
        "    train_transforms = Compose([\n",
        "        LoadImaged(keys=[\"image\", \"label\"]),  # Load the images\n",
        "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),  # Ensure the channel is the first dimension of the image\n",
        "        Spacingd(keys=[\"image\", \"label\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),  # Resample the images\n",
        "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),  # Change the orientation of the image\n",
        "        ScaleIntensityRanged(keys=[\"image\"], a_min=-200, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
        "        # Change the contrast of the image and gives the image pixels,\n",
        "        # values between 0 and 1\n",
        "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\", allow_smaller=True),\n",
        "\n",
        "        RandAffined(\n",
        "            keys=[\"image\", \"label\"],\n",
        "            prob=0.7,\n",
        "            translate_range=(10, 10, 5),\n",
        "            rotate_range=(0, 0, np.pi / 15),\n",
        "            scale_range=(0.1, 0.1, 0.1),\n",
        "            mode=(\"bilinear\", \"nearest\")\n",
        "        ),\n",
        "        RandGaussianNoised(keys=\"image\", prob=0.5),\n",
        "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
        "\n",
        "        Resized(keys=[\"image\", \"label\"], spatial_size=spatial_size),  # Resize the image\n",
        "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "        ToTensord(keys=[\"image\", \"label\"]),  # Convert the images to tensors\n",
        "    ])\n",
        "\n",
        "    # Transforms for the testing\n",
        "    test_transforms = Compose(# Compose transforms together\n",
        "        [\n",
        "            LoadImaged(keys=[\"image\", \"label\"]),  # Load the images\n",
        "            EnsureChannelFirstd(keys=[\"image\", \"label\"]),  # Ensure the channel is the first dimension of the image\n",
        "            Spacingd(keys=[\"image\", \"label\"], pixdim=pixdim, mode=(\"bilinear\", \"nearest\")),\n",
        "            # Resample the images\n",
        "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),  # Change the orientation of the image\n",
        "            ScaleIntensityRanged(keys=[\"image\"], a_min=-200, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
        "            # Change the contrast of the image and gives the image pixels,\n",
        "            # values between 0 and 1\n",
        "            CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\", allow_smaller=True),  # Crop foreground of the image\n",
        "            Resized(keys=[\"image\", \"label\"], spatial_size=spatial_size),  # Resize the image\n",
        "            EnsureTyped(keys=[\"image\", \"label\"]),\n",
        "            ToTensord(keys=[\"image\", \"label\"]),  # Convert the images to tensors\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Create the datasets\n",
        "    train_ds = CacheDataset(data=train_files, transform=train_transforms)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size)\n",
        "\n",
        "    test_ds = CacheDataset(data=test_files, transform=test_transforms)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "id": "8d8f5fc427d9fc7e"
    },
    {
      "cell_type": "markdown",
      "id": "33a723d6",
      "metadata": {
        "id": "33a723d6"
      },
      "source": [
        "##### Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "id": "b5586891",
      "metadata": {
        "id": "b5586891",
        "outputId": "ab7e9f80-1347-4ed9-acb6-b3c64f3fc3e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Save the metadata of the entire training set\n",
        "data_in = preprocess_data(\n",
        "    nif_path,\n",
        "    batch_size=2,  # start conservative\n",
        "    spatial_size=(128, 128, 32),\n",
        "    pixdim=(0.7871384, 0.7871384, 1.2131842)\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading dataset: 100%|██████████| 748/748 [15:08<00:00,  1.21s/it]\n",
            "Loading dataset: 100%|██████████| 240/240 [06:52<00:00,  1.72s/it]\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "id": "15b1f3a791137aae",
      "metadata": {
        "id": "15b1f3a791137aae"
      },
      "source": [
        "##### Setting the device for training"
      ]
    },
    {
      "cell_type": "code",
      "id": "4425e0a7f3020fad",
      "metadata": {
        "id": "4425e0a7f3020fad",
        "outputId": "cbdef050-c006-47d3-a36d-9ccdc856db45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We do the training on the GPU\n",
        "device = torch.device(\"cuda:0\")\n",
        "print(device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "id": "40ff691e13ece641",
      "metadata": {
        "id": "40ff691e13ece641"
      },
      "source": [
        "##### Initialize the model"
      ]
    },
    {
      "cell_type": "code",
      "id": "d8ad17af36631361",
      "metadata": {
        "id": "d8ad17af36631361",
        "outputId": "84ddc689-cdf4-4261-bb04-edf610830629",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = UNet(\n",
        "  spatial_dims=3,\n",
        "  in_channels=1,\n",
        "  out_channels=2,\n",
        "  channels=(16, 32, 64, 128),\n",
        "  strides=(2, 2, 2, 2),\n",
        "  num_res_units=2,\n",
        "  norm=Norm.BATCH,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "model = model.to(device)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/monai/networks/nets/unet.py:130: UserWarning: `len(strides) > len(channels) - 1`, the last 1 values of strides will not be used.\n",
            "  warnings.warn(f\"`len(strides) > len(channels) - 1`, the last {delta} values of strides will not be used.\")\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "id": "156e8458c7fb1dc7",
      "metadata": {
        "id": "156e8458c7fb1dc7"
      },
      "source": [
        "##### Initialize the loss function and the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "id": "2a79dfc6615d0868",
      "metadata": {
        "id": "2a79dfc6615d0868"
      },
      "source": [
        "loss_function = DiceFocalLoss(to_onehot_y=True, softmax=True, lambda_focal=0.5)\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5, amsgrad=True)"
      ],
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_metric(y_pred, y):\n",
        "    dice_loss = DiceLoss(to_onehot_y=True, sigmoid=True, squared_pred=True)\n",
        "    dice_coeff = 1 - dice_loss(y_pred, y).item()\n",
        "    return dice_coeff"
      ],
      "metadata": {
        "id": "IMrU0c6yQCf0"
      },
      "id": "IMrU0c6yQCf0",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1506715d8d977110"
      },
      "cell_type": "markdown",
      "source": [
        "##### Define the training loop"
      ],
      "id": "1506715d8d977110"
    },
    {
      "metadata": {
        "id": "349d3416d78afc91"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": 10,
      "source": [
        "# Function for training the model\n",
        "def train(model, data_in, loss_function, optimizer, max_epochs, model_dir, test_interval=1,\n",
        "          device=torch.device('cuda:0')):\n",
        "    best_metric = -1\n",
        "    best_metric_epoch = -1\n",
        "    save_loss_train = []\n",
        "    save_loss_test = []\n",
        "    save_metric_train = []\n",
        "    save_metric_test = []\n",
        "    train_loader, test_loader = data_in\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        train_epoch_loss = 0\n",
        "        train_step = 0\n",
        "        train_epoch_metric = 0\n",
        "\n",
        "        for batch_data in train_loader:\n",
        "            train_step += 1\n",
        "            volumes = batch_data[\"image\"]\n",
        "            labels = batch_data[\"label\"]\n",
        "            labels = labels != 0\n",
        "            volumes = volumes.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(volumes)\n",
        "\n",
        "            train_loss = loss_function(outputs, labels)\n",
        "\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_epoch_loss += train_loss.item()\n",
        "            train_metric = dice_metric(outputs, labels)\n",
        "            train_epoch_metric += train_metric\n",
        "\n",
        "            print(\n",
        "                f\"{epoch + 1}/{max_epochs} and {train_step}/{len(train_loader)} => train_loss: {train_loss.item():.4f} and train_metric: {train_metric:.4f}\")\n",
        "\n",
        "        print('Saving training data after epoch: ' + str(epoch + 1))\n",
        "        train_epoch_loss /= train_step\n",
        "        print(f\"epoch {epoch + 1} average training loss: {train_epoch_loss:.4f}\")\n",
        "        save_loss_train.append(train_epoch_loss)\n",
        "        np.save(os.path.join(model_dir, 'train_loss.npy'), save_loss_train)\n",
        "\n",
        "        train_epoch_metric /= train_step\n",
        "        print(f\"epoch {epoch + 1} average training metric: {train_epoch_metric:.4f}\")\n",
        "        save_metric_train.append(train_epoch_metric)\n",
        "        np.save(os.path.join(model_dir, 'train_metric.npy'), save_metric_train)\n",
        "\n",
        "        if (epoch + 1) % test_interval == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                test_epoch_loss = 0\n",
        "                test_metric = 0\n",
        "                test_step = 0\n",
        "                test_epoch_metric = 0\n",
        "\n",
        "                for test_data in test_loader:\n",
        "                    test_step += 1\n",
        "                    volumes = test_data[\"image\"]\n",
        "                    labels = test_data[\"label\"]\n",
        "                    labels = labels != 0\n",
        "                    volumes = volumes.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    outputs = model(volumes)\n",
        "\n",
        "                    test_loss = loss_function(outputs, labels)\n",
        "\n",
        "                    test_epoch_loss += test_loss.item()\n",
        "                    test_metric = dice_metric(outputs, labels)\n",
        "                    test_epoch_metric += test_metric\n",
        "\n",
        "                    print(\n",
        "                        f\"{epoch + 1}/{max_epochs} and {test_step}/{len(test_loader)} => test_loss: {test_loss.item():.4f} and test_metric: {test_metric:.4f}\")\n",
        "\n",
        "                print('Saving testing data after epoch: ' + str(epoch + 1))\n",
        "                test_epoch_loss /= test_step\n",
        "                print(f\"epoch {epoch + 1} average testing loss: {test_epoch_loss:.4f}\")\n",
        "                save_loss_test.append(test_epoch_loss)\n",
        "                np.save(os.path.join(model_dir, 'test_loss.npy'), save_loss_test)\n",
        "\n",
        "                test_epoch_metric /= test_step\n",
        "                print(f\"epoch {epoch + 1} average testing metric: {test_epoch_metric:.4f}\")\n",
        "                save_metric_test.append(test_epoch_metric)\n",
        "                np.save(os.path.join(model_dir, 'test_metric.npy'), save_metric_test)\n",
        "\n",
        "                if test_epoch_metric > best_metric:\n",
        "                    best_metric = test_epoch_metric\n",
        "                    best_metric_epoch = epoch + 1\n",
        "                    torch.save(model.state_dict(), os.path.join(model_dir, \"best_metric_model.pth\"))\n",
        "\n",
        "                print(f\"current epoch: {epoch + 1} current test Dice coefficient: {test_epoch_metric:.4f}\"\n",
        "                      f\"\\nbest metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
        "\n",
        "    print(f\"train completed => best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")"
      ],
      "id": "349d3416d78afc91"
    },
    {
      "cell_type": "markdown",
      "id": "8b4ab43200d91d61",
      "metadata": {
        "id": "8b4ab43200d91d61"
      },
      "source": [
        "##### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "id": "d1a67f1a5a944836",
      "metadata": {
        "id": "d1a67f1a5a944836",
        "outputId": "1f9bd9dc-0677-4410-d23b-5bc5d5a31d99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_dir = 'drive/MyDrive/trained_models/post_training_UNet_128_128_32'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "train(model=model,\n",
        "      data_in=data_in,\n",
        "      loss_function=loss_function,\n",
        "      optimizer=optimizer,\n",
        "      max_epochs=100,\n",
        "      model_dir=model_dir,\n",
        "      test_interval=4,\n",
        "      device=device\n",
        ")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/100 and 1/374 => train_loss: 0.7478 and train_metric: 0.4848\n",
            "1/100 and 2/374 => train_loss: 0.7853 and train_metric: 0.4253\n",
            "1/100 and 3/374 => train_loss: 0.7504 and train_metric: 0.4752\n",
            "1/100 and 4/374 => train_loss: 0.7062 and train_metric: 0.5287\n",
            "1/100 and 5/374 => train_loss: 0.6896 and train_metric: 0.5479\n",
            "1/100 and 6/374 => train_loss: 0.6671 and train_metric: 0.5703\n",
            "1/100 and 7/374 => train_loss: 0.6974 and train_metric: 0.5249\n",
            "1/100 and 8/374 => train_loss: 0.7668 and train_metric: 0.4358\n",
            "1/100 and 9/374 => train_loss: 0.7163 and train_metric: 0.4983\n",
            "1/100 and 10/374 => train_loss: 0.6654 and train_metric: 0.5612\n",
            "1/100 and 11/374 => train_loss: 0.6261 and train_metric: 0.6050\n",
            "1/100 and 12/374 => train_loss: 0.6496 and train_metric: 0.5702\n",
            "1/100 and 13/374 => train_loss: 0.7656 and train_metric: 0.4219\n",
            "1/100 and 14/374 => train_loss: 0.7516 and train_metric: 0.4438\n",
            "1/100 and 15/374 => train_loss: 0.7054 and train_metric: 0.5019\n",
            "1/100 and 16/374 => train_loss: 0.6220 and train_metric: 0.5920\n",
            "1/100 and 17/374 => train_loss: 0.5721 and train_metric: 0.6386\n",
            "1/100 and 18/374 => train_loss: 0.5722 and train_metric: 0.6327\n",
            "1/100 and 19/374 => train_loss: 0.7253 and train_metric: 0.4632\n",
            "1/100 and 20/374 => train_loss: 0.7374 and train_metric: 0.4501\n",
            "1/100 and 21/374 => train_loss: 0.6511 and train_metric: 0.5514\n",
            "1/100 and 22/374 => train_loss: 0.5730 and train_metric: 0.6331\n",
            "1/100 and 23/374 => train_loss: 0.6622 and train_metric: 0.5306\n",
            "1/100 and 24/374 => train_loss: 0.7419 and train_metric: 0.4440\n",
            "1/100 and 25/374 => train_loss: 0.6445 and train_metric: 0.5536\n",
            "1/100 and 26/374 => train_loss: 0.5384 and train_metric: 0.6592\n",
            "1/100 and 27/374 => train_loss: 0.5641 and train_metric: 0.6344\n",
            "1/100 and 28/374 => train_loss: 0.7135 and train_metric: 0.4718\n",
            "1/100 and 29/374 => train_loss: 0.6798 and train_metric: 0.5176\n",
            "1/100 and 30/374 => train_loss: 0.5462 and train_metric: 0.6516\n",
            "1/100 and 31/374 => train_loss: 0.5349 and train_metric: 0.6569\n",
            "1/100 and 32/374 => train_loss: 0.6878 and train_metric: 0.4980\n",
            "1/100 and 33/374 => train_loss: 0.7339 and train_metric: 0.4369\n",
            "1/100 and 34/374 => train_loss: 0.6726 and train_metric: 0.5077\n",
            "1/100 and 35/374 => train_loss: 0.5972 and train_metric: 0.5884\n",
            "1/100 and 36/374 => train_loss: 0.5155 and train_metric: 0.6687\n",
            "1/100 and 37/374 => train_loss: 0.4851 and train_metric: 0.6971\n",
            "1/100 and 38/374 => train_loss: 0.5398 and train_metric: 0.6432\n",
            "1/100 and 39/374 => train_loss: 0.7458 and train_metric: 0.4227\n",
            "1/100 and 40/374 => train_loss: 0.7043 and train_metric: 0.4719\n",
            "1/100 and 41/374 => train_loss: 0.6169 and train_metric: 0.5642\n",
            "1/100 and 42/374 => train_loss: 0.5440 and train_metric: 0.6348\n",
            "1/100 and 43/374 => train_loss: 0.5132 and train_metric: 0.6647\n",
            "1/100 and 44/374 => train_loss: 0.5061 and train_metric: 0.6736\n",
            "1/100 and 45/374 => train_loss: 0.5942 and train_metric: 0.5816\n",
            "1/100 and 46/374 => train_loss: 0.7252 and train_metric: 0.4420\n",
            "1/100 and 47/374 => train_loss: 0.6632 and train_metric: 0.5086\n",
            "1/100 and 48/374 => train_loss: 0.5866 and train_metric: 0.5855\n",
            "1/100 and 49/374 => train_loss: 0.5485 and train_metric: 0.6241\n",
            "1/100 and 50/374 => train_loss: 0.5327 and train_metric: 0.6385\n",
            "1/100 and 51/374 => train_loss: 0.6096 and train_metric: 0.5577\n",
            "1/100 and 52/374 => train_loss: 0.7274 and train_metric: 0.4406\n",
            "1/100 and 53/374 => train_loss: 0.6498 and train_metric: 0.5271\n",
            "1/100 and 54/374 => train_loss: 0.5616 and train_metric: 0.6149\n",
            "1/100 and 55/374 => train_loss: 0.4742 and train_metric: 0.6919\n",
            "1/100 and 56/374 => train_loss: 0.4854 and train_metric: 0.6791\n",
            "1/100 and 57/374 => train_loss: 0.6606 and train_metric: 0.5080\n",
            "1/100 and 58/374 => train_loss: 0.6299 and train_metric: 0.5294\n",
            "1/100 and 59/374 => train_loss: 0.6574 and train_metric: 0.5042\n",
            "1/100 and 60/374 => train_loss: 0.6643 and train_metric: 0.5033\n",
            "1/100 and 61/374 => train_loss: 0.5637 and train_metric: 0.6064\n",
            "1/100 and 62/374 => train_loss: 0.4885 and train_metric: 0.6705\n",
            "1/100 and 63/374 => train_loss: 0.4760 and train_metric: 0.6847\n",
            "1/100 and 64/374 => train_loss: 0.5501 and train_metric: 0.6132\n",
            "1/100 and 65/374 => train_loss: 0.7040 and train_metric: 0.4618\n",
            "1/100 and 66/374 => train_loss: 0.7141 and train_metric: 0.4536\n",
            "1/100 and 67/374 => train_loss: 0.6366 and train_metric: 0.5336\n",
            "1/100 and 68/374 => train_loss: 0.5778 and train_metric: 0.5932\n",
            "1/100 and 69/374 => train_loss: 0.5099 and train_metric: 0.6514\n",
            "1/100 and 70/374 => train_loss: 0.4593 and train_metric: 0.6951\n",
            "1/100 and 71/374 => train_loss: 0.6558 and train_metric: 0.5033\n",
            "1/100 and 72/374 => train_loss: 0.5323 and train_metric: 0.6266\n",
            "1/100 and 73/374 => train_loss: 0.7203 and train_metric: 0.4383\n",
            "1/100 and 74/374 => train_loss: 0.6244 and train_metric: 0.5419\n",
            "1/100 and 75/374 => train_loss: 0.5227 and train_metric: 0.6548\n",
            "1/100 and 76/374 => train_loss: 0.5259 and train_metric: 0.6393\n",
            "1/100 and 77/374 => train_loss: 0.6505 and train_metric: 0.5118\n",
            "1/100 and 78/374 => train_loss: 0.5742 and train_metric: 0.5864\n",
            "1/100 and 79/374 => train_loss: 0.5038 and train_metric: 0.6538\n",
            "1/100 and 80/374 => train_loss: 0.6030 and train_metric: 0.5470\n",
            "1/100 and 81/374 => train_loss: 0.7114 and train_metric: 0.4469\n",
            "1/100 and 82/374 => train_loss: 0.7243 and train_metric: 0.4330\n",
            "1/100 and 83/374 => train_loss: 0.6916 and train_metric: 0.4665\n",
            "1/100 and 84/374 => train_loss: 0.6209 and train_metric: 0.5355\n",
            "1/100 and 85/374 => train_loss: 0.5463 and train_metric: 0.6074\n",
            "1/100 and 86/374 => train_loss: 0.4909 and train_metric: 0.6592\n",
            "1/100 and 87/374 => train_loss: 0.4533 and train_metric: 0.6921\n",
            "1/100 and 88/374 => train_loss: 0.5315 and train_metric: 0.6174\n",
            "1/100 and 89/374 => train_loss: 0.7068 and train_metric: 0.4465\n",
            "1/100 and 90/374 => train_loss: 0.6861 and train_metric: 0.4704\n",
            "1/100 and 91/374 => train_loss: 0.6097 and train_metric: 0.5465\n",
            "1/100 and 92/374 => train_loss: 0.5261 and train_metric: 0.6255\n",
            "1/100 and 93/374 => train_loss: 0.4793 and train_metric: 0.6675\n",
            "1/100 and 94/374 => train_loss: 0.4605 and train_metric: 0.6807\n",
            "1/100 and 95/374 => train_loss: 0.6012 and train_metric: 0.5490\n",
            "1/100 and 96/374 => train_loss: 0.7107 and train_metric: 0.4397\n",
            "1/100 and 97/374 => train_loss: 0.6396 and train_metric: 0.5108\n",
            "1/100 and 98/374 => train_loss: 0.5550 and train_metric: 0.5952\n",
            "1/100 and 99/374 => train_loss: 0.4929 and train_metric: 0.6527\n",
            "1/100 and 100/374 => train_loss: 0.4674 and train_metric: 0.6754\n",
            "1/100 and 101/374 => train_loss: 0.4371 and train_metric: 0.7017\n",
            "1/100 and 102/374 => train_loss: 0.5191 and train_metric: 0.6262\n",
            "1/100 and 103/374 => train_loss: 0.6874 and train_metric: 0.4606\n",
            "1/100 and 104/374 => train_loss: 0.5547 and train_metric: 0.5954\n",
            "1/100 and 105/374 => train_loss: 0.4090 and train_metric: 0.7285\n",
            "1/100 and 106/374 => train_loss: 0.4542 and train_metric: 0.6856\n",
            "1/100 and 107/374 => train_loss: 0.6994 and train_metric: 0.4494\n",
            "1/100 and 108/374 => train_loss: 0.5786 and train_metric: 0.5696\n",
            "1/100 and 109/374 => train_loss: 0.4600 and train_metric: 0.6784\n",
            "1/100 and 110/374 => train_loss: 0.5126 and train_metric: 0.6277\n",
            "1/100 and 111/374 => train_loss: 0.6996 and train_metric: 0.4501\n",
            "1/100 and 112/374 => train_loss: 0.5572 and train_metric: 0.5935\n",
            "1/100 and 113/374 => train_loss: 0.4484 and train_metric: 0.6941\n",
            "1/100 and 114/374 => train_loss: 0.3936 and train_metric: 0.7355\n",
            "1/100 and 115/374 => train_loss: 0.5557 and train_metric: 0.5867\n",
            "1/100 and 116/374 => train_loss: 0.5968 and train_metric: 0.5441\n",
            "1/100 and 117/374 => train_loss: 0.7050 and train_metric: 0.4492\n",
            "1/100 and 118/374 => train_loss: 0.5229 and train_metric: 0.6207\n",
            "1/100 and 119/374 => train_loss: 0.4189 and train_metric: 0.7166\n",
            "1/100 and 120/374 => train_loss: 0.5158 and train_metric: 0.6087\n",
            "1/100 and 121/374 => train_loss: 0.5197 and train_metric: 0.6240\n",
            "1/100 and 122/374 => train_loss: 0.5433 and train_metric: 0.5856\n",
            "1/100 and 123/374 => train_loss: 0.5813 and train_metric: 0.5674\n",
            "1/100 and 124/374 => train_loss: 0.4259 and train_metric: 0.7086\n",
            "1/100 and 125/374 => train_loss: 0.4312 and train_metric: 0.7026\n",
            "1/100 and 126/374 => train_loss: 0.6016 and train_metric: 0.5426\n",
            "1/100 and 127/374 => train_loss: 0.4392 and train_metric: 0.6934\n",
            "1/100 and 128/374 => train_loss: 0.4035 and train_metric: 0.7257\n",
            "1/100 and 129/374 => train_loss: 0.6600 and train_metric: 0.5034\n",
            "1/100 and 130/374 => train_loss: 0.4185 and train_metric: 0.7075\n",
            "1/100 and 131/374 => train_loss: 0.4287 and train_metric: 0.6962\n",
            "1/100 and 132/374 => train_loss: 0.6402 and train_metric: 0.5215\n",
            "1/100 and 133/374 => train_loss: 0.4577 and train_metric: 0.6860\n",
            "1/100 and 134/374 => train_loss: 0.3344 and train_metric: 0.7785\n",
            "1/100 and 135/374 => train_loss: 0.4927 and train_metric: 0.6169\n",
            "1/100 and 136/374 => train_loss: 0.6006 and train_metric: 0.5483\n",
            "1/100 and 137/374 => train_loss: 0.4031 and train_metric: 0.7224\n",
            "1/100 and 138/374 => train_loss: 0.5677 and train_metric: 0.5632\n",
            "1/100 and 139/374 => train_loss: 0.5031 and train_metric: 0.6377\n",
            "1/100 and 140/374 => train_loss: 0.3525 and train_metric: 0.7611\n",
            "1/100 and 141/374 => train_loss: 0.5176 and train_metric: 0.6095\n",
            "1/100 and 142/374 => train_loss: 0.4762 and train_metric: 0.6586\n",
            "1/100 and 143/374 => train_loss: 0.3588 and train_metric: 0.7615\n",
            "1/100 and 144/374 => train_loss: 0.5748 and train_metric: 0.5607\n",
            "1/100 and 145/374 => train_loss: 0.5053 and train_metric: 0.6191\n",
            "1/100 and 146/374 => train_loss: 0.3325 and train_metric: 0.7764\n",
            "1/100 and 147/374 => train_loss: 0.3842 and train_metric: 0.7315\n",
            "1/100 and 148/374 => train_loss: 0.6375 and train_metric: 0.5148\n",
            "1/100 and 149/374 => train_loss: 0.5855 and train_metric: 0.5664\n",
            "1/100 and 150/374 => train_loss: 0.6248 and train_metric: 0.5333\n",
            "1/100 and 151/374 => train_loss: 0.5010 and train_metric: 0.6412\n",
            "1/100 and 152/374 => train_loss: 0.5440 and train_metric: 0.5780\n",
            "1/100 and 153/374 => train_loss: 0.6562 and train_metric: 0.4892\n",
            "1/100 and 154/374 => train_loss: 0.5773 and train_metric: 0.5654\n",
            "1/100 and 155/374 => train_loss: 0.4967 and train_metric: 0.6430\n",
            "1/100 and 156/374 => train_loss: 0.3910 and train_metric: 0.7328\n",
            "1/100 and 157/374 => train_loss: 0.3865 and train_metric: 0.7307\n",
            "1/100 and 158/374 => train_loss: 0.7228 and train_metric: 0.4354\n",
            "1/100 and 159/374 => train_loss: 0.5497 and train_metric: 0.6085\n",
            "1/100 and 160/374 => train_loss: 0.3492 and train_metric: 0.7706\n",
            "1/100 and 161/374 => train_loss: 0.5497 and train_metric: 0.5814\n",
            "1/100 and 162/374 => train_loss: 0.5733 and train_metric: 0.5794\n",
            "1/100 and 163/374 => train_loss: 0.3718 and train_metric: 0.7476\n",
            "1/100 and 164/374 => train_loss: 0.4332 and train_metric: 0.6864\n",
            "1/100 and 165/374 => train_loss: 0.6912 and train_metric: 0.4595\n",
            "1/100 and 166/374 => train_loss: 0.5272 and train_metric: 0.6084\n",
            "1/100 and 167/374 => train_loss: 0.3658 and train_metric: 0.7456\n",
            "1/100 and 168/374 => train_loss: 0.5458 and train_metric: 0.5750\n",
            "1/100 and 169/374 => train_loss: 0.5279 and train_metric: 0.6136\n",
            "1/100 and 170/374 => train_loss: 0.4248 and train_metric: 0.7216\n",
            "1/100 and 171/374 => train_loss: 0.3705 and train_metric: 0.7468\n",
            "1/100 and 172/374 => train_loss: 0.6223 and train_metric: 0.5214\n",
            "1/100 and 173/374 => train_loss: 0.4722 and train_metric: 0.6598\n",
            "1/100 and 174/374 => train_loss: 0.3640 and train_metric: 0.7595\n",
            "1/100 and 175/374 => train_loss: 0.5941 and train_metric: 0.5418\n",
            "1/100 and 176/374 => train_loss: 0.4766 and train_metric: 0.6410\n",
            "1/100 and 177/374 => train_loss: 0.4822 and train_metric: 0.6208\n",
            "1/100 and 178/374 => train_loss: 0.4694 and train_metric: 0.6524\n",
            "1/100 and 179/374 => train_loss: 0.4936 and train_metric: 0.6303\n",
            "1/100 and 180/374 => train_loss: 0.3679 and train_metric: 0.7449\n",
            "1/100 and 181/374 => train_loss: 0.7217 and train_metric: 0.4388\n",
            "1/100 and 182/374 => train_loss: 0.5666 and train_metric: 0.5784\n",
            "1/100 and 183/374 => train_loss: 0.6059 and train_metric: 0.5406\n",
            "1/100 and 184/374 => train_loss: 0.6827 and train_metric: 0.4790\n",
            "1/100 and 185/374 => train_loss: 0.4110 and train_metric: 0.7158\n",
            "1/100 and 186/374 => train_loss: 0.6081 and train_metric: 0.5387\n",
            "1/100 and 187/374 => train_loss: 0.5762 and train_metric: 0.5779\n",
            "1/100 and 188/374 => train_loss: 0.5286 and train_metric: 0.5864\n",
            "1/100 and 189/374 => train_loss: 0.6681 and train_metric: 0.4850\n",
            "1/100 and 190/374 => train_loss: 0.5539 and train_metric: 0.5861\n",
            "1/100 and 191/374 => train_loss: 0.4386 and train_metric: 0.6865\n",
            "1/100 and 192/374 => train_loss: 0.3467 and train_metric: 0.7628\n",
            "1/100 and 193/374 => train_loss: 0.3165 and train_metric: 0.7902\n",
            "1/100 and 194/374 => train_loss: 0.3093 and train_metric: 0.7935\n",
            "1/100 and 195/374 => train_loss: 0.4252 and train_metric: 0.6899\n",
            "1/100 and 196/374 => train_loss: 0.6682 and train_metric: 0.4813\n",
            "1/100 and 197/374 => train_loss: 0.5302 and train_metric: 0.5985\n",
            "1/100 and 198/374 => train_loss: 0.4547 and train_metric: 0.6797\n"
          ]
        }
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}